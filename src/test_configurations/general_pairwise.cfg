#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
A general pairwise (e.g. A is better than B) configuration for evaluating overall quality
of a set of audio stimuli.
"""
import itertools

# ---------------------------------------------------------------------------------------------
# TESTING VARIABLES
TEST_TYPE = 'pairwise'

# ---------------------------------------------------------------------------------------------
# FRONT-END VARIABLES
TEST_TIMEOUT_SEC = 5.
PREVIEW_HTML = """
    <p>This listening test aims to rate the quality of a set of signals in comparison to a reference signal.</p>

    <p>During the test, you will be asked to assess the quality of the audio. The test will have two main parts:
    1) a training phase and 2) an evaluation phase. During the training phase, you
    will be trained on example ratings of quality. During the evaluation phase, you will listen to several of pairs
     of recordings and choose which recording in each pair has higher quality.</p>

    <p><b>You may receive up to a $0.25 bonus based on the consistency of your ratings.</b></p>

    <p>The expected total duration of the test is 5-7 minutes.</p>

    <p>However, if this is your first HIT in this group, there will also be additional surveys and hearing tests, which
    will increase the expected total duration of the first HIT is 8-10 minutes. Because this first HIT takes longer than
    the rest of the HITs in this group, you will be given an additional $0.30 bonus.</p>
    """

# ---------------------------------------------------------------------------------------------
# DEFAULT CONDITION AND TEST-SPECIFIC VARIABLES
#   (These will be configured for each condition and saved in the database)
test_title = 'Task: Rate the <quality>'

first_task_introduction_html = """
    <p>This listening test aims to rate the quality of a set of signals in comparison to a reference signal.</p>

    <p>During the training phase, you will be trained on example ratings of quality. During the evaluation phase,
    you will listen to several of pairs of recordings and choose which recording in each pair has higher <quality>.
    Lastly, at the end, there will be a short survey which you will only have to fill out for the first HIT you
    complete.</p>
    """

introduction_html = """
    <p>This listening test aims to rate the quality of a set of signals in comparison to a reference signal.</p>

    <p>During the training phase, you will be trained on example ratings of quality. During the evaluation phase, you
    will listen to several of pairs of recordings and choose which recording in each pair has higher quality.</p>
    """

training_instructions_html = """
    <p><quality_explanation></p>

    <p>Instructions:
    <ol>
    <li>If you have not done so already, set the volume of your headphones/speakers so that
    it's comfortable. You should be able to clearly hear differences between recordings
    (the volume shouldn't be changed later on).</li>
    <li>Train yourself on the example recordings to learn what types of recordings we
    expect to be higher or lower than average according to the quality scale.</li>
    </ol>
    </p>
    """

evaluation_instructions_html = """
    <p>Listen to the Reference recording below by clicking on its labeled
    buttons. The Reference recording will be the same for all pairs of
    A/B recordings.</p>

    <p>Listen to recordings A and B, and select which of the two recordings you would rate
    as having higher <quality>. <quality_explanation></p>

    <p>You can listen to the recordings as many times as you want before deciding.</p>
    """

num_audio_files = 10

references = (('Reference', 'The reference signal to which test signals are compared.'),)

# The quality scales
qualities = ['<b>overall quality</b> compared to the reference sound for each recording', ]

# Descriptions of the quality scales
quality_explanations = ['<b>Overall quality</b> refers to overall quality of a recording based on how similar it is'
                        ' to the reference sound. For example, if a recording is very similar to the the reference,'
                        ' it should be given a high rating. However, if a recording is very different from the '
                        'reference sound (e.g. because of degradation or additional sounds), it should be given a low'
                        ' rating.', ]

# ---------------------------------------------------------------------------------------------
# TRAINING EXAMPLES FOR PARTICIPANTS
# These are the reference examples for the training examples
# List entries must be dicts composed as {<reference_name>: <example_url>, ...}
reference_example_dicts = [{'Reference': '/audio/exp00_target.wav', }, ]

# These are the quality examples.
# List entries should be dicts composed as {<description>: <example_url>, ...}
quality_example_dicts = [{'Low': ['/audio/exp00_anchorArtif.wav',
                                  '/audio/exp00_anchorDistTarget.wav'],
                          'High': ['/audio/exp00_target.wav', ]}, ]

TESTS = []
conditions = []
for quality, quality_explanation, reference_example_dict, quality_example_dict in zip(qualities,
                                                                                      quality_explanations,
                                                                                      reference_example_dicts,
                                                                                      quality_example_dicts, ):
    test_cfg_vars = {}
    test_cfg_vars['references'] = references

    test_cfg_vars['reference_example_dict'] = reference_example_dict
    test_cfg_vars['quality_example_dict'] = quality_example_dict

    test_cfg_vars['test_title'] = test_title
    test_cfg_vars['test_title'] = test_cfg_vars['test_title'].replace('<quality>', quality)

    test_cfg_vars['introduction_html'] = introduction_html
    test_cfg_vars['introduction_html'] = test_cfg_vars['introduction_html'].replace('<quality>', quality)
    test_cfg_vars['introduction_html'] = test_cfg_vars['introduction_html'].replace('<quality_explanation>', quality_explanation)

    test_cfg_vars['first_task_introduction_html'] = first_task_introduction_html
    test_cfg_vars['first_task_introduction_html'] = test_cfg_vars['first_task_introduction_html'].replace('<quality>', quality)
    test_cfg_vars['first_task_introduction_html'] = test_cfg_vars['first_task_introduction_html'].replace('<quality_explanation>', quality_explanation)

    test_cfg_vars['training_instructions_html'] = training_instructions_html
    test_cfg_vars['training_instructions_html'] = test_cfg_vars['training_instructions_html'].replace('<quality>', quality)
    test_cfg_vars['training_instructions_html'] = test_cfg_vars['training_instructions_html'].replace('<quality_explanation>', quality_explanation)

    test_cfg_vars['evaluation_instructions_html'] = evaluation_instructions_html
    test_cfg_vars['evaluation_instructions_html'] = test_cfg_vars['evaluation_instructions_html'].replace('<quality>', quality)
    test_cfg_vars['evaluation_instructions_html'] = test_cfg_vars['evaluation_instructions_html'].replace('<quality_explanation>', quality_explanation)

    test = {'test_config_variables': test_cfg_vars,
            'condition_groups': []}

    for i in range(1, num_audio_files + 1):
        # THE AUDIO STIMULUS FILES
        group_data = {'reference_files': [['Reference', 'exp%02d_target.wav' % i, ]],
                      'stimulus_files': [['S1', 'exp%02d_target.wav' % i],
                                         ['S2', 'exp%02d_anchorArtif.wav' % i],
                                         ['S3', 'exp%02d_anchorDistTarget.wav' % i],
                                         ['S4', 'exp%02d_test5.wav' % i],
                                         ['S5', 'exp%02d_test6.wav' % i],
                                         ['S6', 'exp%02d_test7.wav' % i],
                                         ['S7', 'exp%02d_test8.wav' % i]]}

        conditions = []
        for stimulus_pair in itertools.combinations([l[0] for l in group_data['stimulus_files']], 2):
            condition_data = {'reference_keys': ['Reference', ],
                              'stimulus_keys': stimulus_pair,
                              'evaluation_instructions_html': None}
            conditions.append(condition_data)
        group_data['conditions'] = conditions
        test['condition_groups'].append(group_data)
    TESTS.append(test)

CONDITIONS_PER_EVALUATION = len(conditions) / 2
